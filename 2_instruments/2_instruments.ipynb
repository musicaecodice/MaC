{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e8a8b13",
   "metadata": {},
   "source": [
    "[INDEX](../indice.ipynb) \n",
    "\n",
    "[Genres](#tipi) \n",
    "\n",
    "[Non interactive systems](#noninteract) \n",
    "* [Fixed media music](#fixed) \n",
    "  - [Acousmatic music](#acousma)\n",
    "  - [Wallpaper music, ambient music and soundscapes](#ambient)\n",
    "  - [Radiodrama](#radiodrama)\n",
    "  - [Computer music - algorithmic composition](#cac)\n",
    "* [Live sequencing](#liveseq) \n",
    "  - [Sound design for performing arts or video](#dance)\n",
    "  - [Sound design for gaming](#game)\n",
    "  \n",
    "[Interactive systems](#interact):\n",
    "* [Hyper-instruments](#hiper)\n",
    "* [Live set](#lset) \n",
    "* [Live coding](#lcoding)\n",
    "\n",
    "[The virtual instrument paradigm](#virtual)\n",
    "* [Software installation](#installazione)\n",
    "* [instrumental model](#strum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4a8ed1",
   "metadata": {},
   "source": [
    "# Genres <a id='tipi'></a>\n",
    "\n",
    "Before starting to write an electroacoustic piece we should:\n",
    "\n",
    "* define the instrumentation to be used which can include acoustic instruments, electroacoustic instruments, virtual instruments or a mix of these types.\n",
    "* define the sound amplification and diffusion system.\n",
    "\n",
    "For this reason here we categorize the different types and genres of electroacoustic music by dividing them into two macro areas defined by the way of controlling sounds and musical parameters.\n",
    "\n",
    "* non interactive systems.\n",
    "* interactive systems.\n",
    "\n",
    "We will explore the compositional strategies of each in dedicated chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1a8e5",
   "metadata": {},
   "source": [
    "# Non interactive systems <a id='noninteract'></a>\n",
    "\n",
    "This category includes genres that:\n",
    "\n",
    "* don't involve any human interaction during the performance.  \n",
    "* involve interactions that don't influence the musical or sound text in any way.\n",
    "\n",
    "According with the reflections of the previous chapter we can affirm:\n",
    "\n",
    "* interactions may influence the listener's perception of the sound text (mainly through the different characteristics of the diffusion systems used) \n",
    "* interactions don't change the sound text.\n",
    "\n",
    "This category can be divided into two further subcategories:\n",
    "\n",
    "* fixed media music\n",
    "* live sequencing.\n",
    "\n",
    "The image shows the possibles audio chains in this category.\n",
    "\n",
    "* non-real-time sound processing\n",
    "* non-real-time synthesis.\n",
    "* real-time synthesis.\n",
    "\n",
    "<!---<center><img src=\"img/chains.png\" width=\"100%\"></center>--->\n",
    "\n",
    "<div style=\"width:100%; margin-left: auto; margin-right: auto\">\n",
    "\n",
    "![](/Users/andreavigani/Desktop/GHub/EMC/2_instruments/img/chains.png)\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc7200",
   "metadata": {},
   "source": [
    "## Fixed media music <a id='fixed'></a>\n",
    "\n",
    "This subcategory denotes music or video that is played back from a recording.\n",
    "\n",
    "This used to be called 'tape music' but tapes are no longer used.\n",
    "\n",
    "The entire work is stored on a single medium and reproduced from beginning to end.\n",
    "\n",
    "We can identify three main genres:\n",
    "\n",
    "* Acousmatic music.\n",
    "* Wallpaper music, ambient music and soundscapes. \n",
    "* Radiodrama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0417a71",
   "metadata": {},
   "source": [
    "### Acousmatic music <a id='acousma'></a>\n",
    "\n",
    "<!---<center><img src=\"img/tape.png\" width=\"10%\"></center>--->\n",
    "\n",
    "<div style=\"width:10%; margin-left: auto; margin-right: auto\">\n",
    "\n",
    "![](/Users/andreavigani/Desktop/GHub/EMC/2_instruments/img/tape.png)\n",
    "   \n",
    "</div>\n",
    "\n",
    "We can start from musique concrète.\n",
    "\n",
    "In the 1940s Pierre Schaeffer began composing with the sound (acoustic world). \n",
    "\n",
    "After applying various sound elaborations (editing, filtering, reversal, etc.) he began to apply musical techniques to their combinations (repetition, transposition, reordering, etc.) inventing new musical structures (symbolic world).\n",
    "\n",
    "In doing so, he reversed the compositional practice that had been in force for centuries in the Western musical tradition.\n",
    "\n",
    "The listeners could not make the cognitive associations illustrated in the first chapter.\n",
    "\n",
    "They activating exclusively the basic cognitive mechanisms common to all human beings.\n",
    "\n",
    "This listening mode has been called 'reduced listening'.\n",
    "\n",
    "All listeners of this new music are comparable to the aborigine who listens to a Beethoven symphony.\n",
    "\n",
    "P.Shaeffer - Etude aux chemins de fer (1948).\n",
    "\n",
    "* mono analog tape. \n",
    "* recorded and processed train sounds.\n",
    "* sounds from the acoustic environment.\n",
    "* we can recognize the sound source and its processing.\n",
    "* perceived as a distortion of reality.\n",
    "\n",
    "<audio controls src='suoni/shaeffer.mp3'></audio> \n",
    "\n",
    "Acousmatic music is the modern evolution of musique concrète.\n",
    "\n",
    "By the mid-1970s a number of composers felt a need to designate their conception clearly in terms of specific methodology, syntax, and tools. \n",
    "\n",
    "François Bayle suggested adopting the term acousmatique.\n",
    "\n",
    "This term comes from the Greekakousma: what is heard. \n",
    "\n",
    "Students of Pythagoras listened to his teachings from behind a screen unable to see him. \n",
    "\n",
    "He believed that the lack of visual cues would force his students to focus all their attention on his message. \n",
    "\n",
    "After his death his followers split into:\n",
    "\n",
    "* Acousmatics (practitioners of the mystic doctrine) \n",
    "* Mathematics (scientists).\n",
    "\n",
    "The loudspeaker was the modern equivalent of the screen partition.\n",
    "\n",
    "\"From  an  esthetical  point  of  view  acousmatic  music  concentrates  on  the  poetical  and  spectral richness of sounds, and plays with this very particular characteristic of sound hearing in which the perception of an acoustic phenomena is associated with its cause; hence  the  perception  of  a  sound  whose  cause  is  unknown  or  unrecognizable  for  our  perception, induces the listener to imagine non-existing causes and to perceive music as a complex creative phenomena in which musical sense and musical sounds have to be  interpreted  simultaneously,  with  generally  very  little  relation  with  our  perceptive  reality. The question is not to find out how sounds are made but how their combination will generate imaginary perceptions of imaginary realities in our mind.\"\n",
    "\n",
    "Teruggi, D. 1995. What about acousmatics? In Journal of Electroacoustic Music. Vol. 7. London: Sonic Arts Network\n",
    "\n",
    "[More info](img/teruggi.pdf)\n",
    " \n",
    "B.Parmegiani - De Natura Sonorum (1975). \n",
    "\n",
    "* stereo digital tape. \n",
    "* recorded and processed sounds plus synthetic sounds.\n",
    "* sounds from an imaginary environment.\n",
    "* we cannot recognize the sound source.\n",
    "* perceived as an alternative or virtual reality.\n",
    "\n",
    "<audio controls src='suoni/parmegiani.mp3'></audio>  \n",
    "\n",
    "Another characteristic of acousmatic music is that it is performed live on loudspeaker orchestras (acousmonium).\n",
    "\n",
    "Indeed Bayle referred to acousmatic musica as “art of projected sounds which is shot and developed in the studio, projected in a hall, like cinema”\n",
    "\n",
    "H.Vaggione - Consort for convolved violins (2011). \n",
    "\n",
    "* multitrack digital file.\n",
    "* recorded and processed instrumental sounds.\n",
    "* sounds from music environment.\n",
    "* we can recognize the sound source and its avatars.\n",
    "* perceived as a piece of music.\n",
    "\n",
    "<audio controls src='suoni/vaggione.mp3'></audio>  \n",
    "\n",
    "[More info](img/space.pdf) about acousmatic works interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1168ee",
   "metadata": {},
   "source": [
    "### Wallpaper music, ambient music and soundscapes <a id='ambient'></a>\n",
    "\n",
    "<!---<center><img src=\"img/ambi.png\" width=\"13%\"></center>--->\n",
    "\n",
    "<div style=\"width:13%; margin-left: auto; margin-right: auto\">\n",
    "\n",
    "![](/Users/andreavigani/Desktop/GHub/EMC/2_instruments/img/ambi.png)\n",
    "   \n",
    "</div>\n",
    "\n",
    "We can start from Eric Satie (non-electroacoustic composer).\n",
    "\n",
    "He was a precursor of minimalism, muzak, and many other 20th-century musical genres. \n",
    "\n",
    "Satie conceived this musical genre as a backdrop to everyday activities (passive listening). \n",
    "\n",
    "Satie was familiar with this idea due to its role as role as a Montmartre café pianist in the late 19th and early 20th century. \n",
    "\n",
    "E.Satie - musique d'ameublement (1917).\n",
    "\n",
    "* acoustic instruments.\n",
    "* pattern iteration.\n",
    "* music environment.\n",
    "* time dilation (psychological time)). \n",
    "* passive and non immersive listening.\n",
    "* perceived as a piece of music.\n",
    "\n",
    "<audio controls src='suoni/satie.mp3'></audio>   \n",
    "\n",
    "We know well how nowadays we are bombarded daily by this type of music in a totally involuntary motion.\n",
    "\n",
    "Let us recall what was stated in the first chapter on the basic cognitive mechanisms of human beings.\n",
    "\n",
    "In the 1950s a visionary composer like J.Cage took this idea to its extreme with the piece 4'33'' whose sonic and musical content consists of the background noise of a finite time.\n",
    "\n",
    "Later in 1978 another composer Brian Eno coined the term ambient as a musical genre.\n",
    "\n",
    "A Sunday morning in the Cologne airport while waiting for a flight he tought: “The light was beautiful, everything was beautiful, except they were playing awful music. They spend hundreds of millions of pounds…on everything. Except the music.” \n",
    "\n",
    "It was this that compelled him to begin composing music for public environments.\n",
    "\n",
    "We must consider that the diffusion of sounds and/or music in environments not designed for active listening can change the perception of that environment in a subconscious way.\n",
    "\n",
    "It is time that through sound redraws the boundaries of a space.\n",
    "\n",
    "B.Eno - Music for Airports (1978).\n",
    "* stereo vynil.\n",
    "* pattern iteration.\n",
    "* music environment.\n",
    "* time dilation (psychological time)). \n",
    "* passive and semi immersive listening.\n",
    "* perceived as a piece of music.\n",
    "\n",
    "<audio controls src='suoni/eno.mp3'></audio>   \n",
    "\n",
    "[More info](img/eno.pdf)\n",
    "\n",
    "Soundscape composition can represent a meeting point between musique concrete and ambient music because:\n",
    "\n",
    "* it uses sound materials recorded from the real world or synthesized.\n",
    "* does not recompose them according to symbolic rules and principles into a musical form.\n",
    "* its purpose is to construct or reconstruct a real or virtual soundscape.\n",
    "\n",
    "It's essentially based on the studies and works of Canadian musicologist and composer Raymond Murray Schafer.\n",
    "\n",
    "To put it simply, we can say that different types of sounds coexist in a sound environment and we can identify three types:\n",
    "\n",
    "* geophony $\\rightarrow$ all the sounds of nature of non-biological origin.\n",
    "* biophony $\\rightarrow$ all the sounds of nature of biological origin but not caused by humans.\n",
    "* anthrophony $\\rightarrow$  all sounds generated by humans.\n",
    "    - language.\n",
    "    - music.\n",
    "    - mechanical sound.\n",
    "\n",
    "Schafer also divides the sounds of a soundscape into two categories:\n",
    "\n",
    "* hi-fi $\\rightarrow$ all sounds that have an hight signal-to-noise ratio (they are distinguishable from the background noise).\n",
    "* lo-fi $\\rightarrow$ all sounds that have a low signal-to-noise ratio (they are indistinguishable from the background noise).\n",
    "\n",
    "Three main elements in a soundscape:\n",
    "\n",
    "* keynote $\\rightarrow$ sounds that most characterize an environment, analogous to the tonality in tonal music.\n",
    "* sound signals $\\rightarrow$ all the foreground sounds that do not necessarily characterize an environment but may appear occasionally.\n",
    "* soundmark $\\rightarrow$ The unique sounds of a given soundscape.\n",
    "\n",
    "N.Barret - Soundwalk (1998).</br>\n",
    "* stereo digital tape.\n",
    "* no pattern.\n",
    "* acoustic environment.\n",
    "* time as in real life (clock time)). \n",
    "* passive and immersive listening.\n",
    "* perceived as an acoustoc environment.\n",
    "\n",
    "<audio controls src='suoni/soundwalk.mp3'></audio>   \n",
    "\n",
    "[More info](img/schafer.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dff02b9",
   "metadata": {},
   "source": [
    "### Radiodrama <a id='radiodrama'></a>\n",
    "\n",
    "<!---<center><img src=\"img/radio.png\" width=\"10%\"></center>--->\n",
    "\n",
    "<div style=\"width:10%; margin-left: auto; margin-right: auto\">\n",
    "\n",
    "![](/Users/andreavigani/Desktop/GHub/EMC/2_instruments/img/radio.png)\n",
    "   \n",
    "</div>\n",
    "\n",
    "Radio drama can use only four kinds of signs: \n",
    "\n",
    "* speech \n",
    "* sound effects \n",
    "* music\n",
    "* silence \n",
    "\n",
    "Any one of these by itself can be a very slippery client to deal with. \n",
    "\n",
    "Silence cane a listener suspect a transmission failure and switch off, or tune over to another on. \n",
    "\n",
    "Sound effects are notoriously misleading - a sneeze can be taken for a bomb explosion! \n",
    "\n",
    "An unfamiliar music will sound irritatingly like cacophony. \n",
    "\n",
    "And speech says nothing to someone who doesn't know what you mean by the words. \n",
    "\n",
    "But if you hit on harmonious combination of the right choies of these four kinds you can move mountains, deploy infantry battalions with Air Force support, immerse a soul in the joys of paradise.\n",
    "\n",
    "In short do anything you please, all in few minutes, one after another.\n",
    "\n",
    "The main focus is narrativity and a continuous balance in the construction of meaning between explicit sounds (both in natural and auditory language) and perceptual hints and/or allusions.\n",
    "\n",
    "O.Wells - War of the worlds (1938).\n",
    "\n",
    "* mono radio signal.\n",
    "* natural language.\n",
    "* didactic sound design to simulate the real acoustic environment.\n",
    "* time as in real life (clock time). \n",
    "* active listening non immersive.\n",
    "* perceived as a representation of reality.\n",
    "\n",
    "<audio controls src='suoni/wells.mp3'></audio>   \n",
    "\n",
    "  [More info](img/radio.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d60639",
   "metadata": {},
   "source": [
    "### Computer music - algorithmic composition <a id='cac'></a>\n",
    "\n",
    "<!---<center><img src=\"img/comp.png\" width=\"10%\"></center>--->\n",
    "\n",
    "<div style=\"width:10%; margin-left: auto; margin-right: auto\">\n",
    "\n",
    "![](/Users/andreavigani/Desktop/GHub/EMC/2_instruments/img/comp.png)\n",
    "   \n",
    "</div>\n",
    "\n",
    "The birth of computer music is naturally linked to the invention of the electronic calculator and the subsequent spread of personal computers as well as the invention of new (formal) languages that allow humans to communicate with these machines.\n",
    "\n",
    "Starting from the second half of the 1950s, two orientations emerged:\n",
    "\n",
    "* The first one (developed mainly in Europe) deals with the digital coding of musical parameters (assisted composition, assisted musical analysis, etc.).\n",
    "* The second one (developed mainly in USA) deals with the digital coding of sound parameters (synthesis and processing).\n",
    "\n",
    "In 1957 MUSIC I was created by Max Mathews in Bell Laboratories. \n",
    "\n",
    "It was the first software for playing music from mathematical functions.\n",
    "\n",
    "After MUSIC I, there were: MUSIC II, III, IV, V, MUSIC 360, and then C-sound and SuperCollider languages still used today.\n",
    "\n",
    "The main sound synthesis and processing techniques mainly used are:\n",
    "\n",
    "* Additive Synthesis\n",
    "* Subctractive synthesis (filters)\n",
    "* FM (Frequency Modulation) Synthesis\n",
    "* Granular Synthesis\n",
    "* Analysis and Resynthesis (FFT)\n",
    "* Generative Algorithms for:\n",
    "  - Structures\n",
    "  - Timbres\n",
    "  - Pitches\n",
    "* Remote Control of Computers (MIDI, OSC, HID)\n",
    "\n",
    "As we all know, nowadays computer is the main musical instrument, like piano in the 19th century.\n",
    "\n",
    "The term 'computer music' as an aesthetic category identifies the musical genre adopted by the pioneers from the mid-50s to the late 90s\n",
    "\n",
    "J.Chowning - Stria (1977).\n",
    "\n",
    "* stereo digital tape.\n",
    "* sonic environment.\n",
    "* sounds from imaginary environment (only synthetic sounds - FM)).\n",
    "* time dilation (psychological time)). \n",
    "* active listening.\n",
    "* perceived as a piece of computer music.\n",
    "\n",
    "<audio controls src='suoni/chowning .mp3'></audio>\n",
    "\n",
    "[More info](img/max.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4036ae82",
   "metadata": {},
   "source": [
    "## Live sequencing <a id='liveseq'></a>\n",
    "\n",
    "This subcategory is almost identical to the previous one except for two aspects: \n",
    "* works can be generated in real time through algorithms (computer music). \n",
    "* tracks are divided into different sound files triggered in real time (sound design for theatre and gaming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8492c440",
   "metadata": {},
   "source": [
    "### Sound design for performing arts or video <a id='dance'></a>\n",
    "\n",
    "<!---<center><img src=\"img/danza.png\" width=\"13%\"></center>--->\n",
    "\n",
    "<div style=\"width:13%; margin-left: auto; margin-right: auto\">\n",
    "\n",
    "![](/Users/andreavigani/Desktop/GHub/EMC/2_instruments/img/danza.png)\n",
    "   \n",
    "</div>\n",
    "\n",
    "Works on fixed media which differ from the works illustrated in the previous section essentially for:\n",
    "\n",
    "* sound does not have an absolute value but must be functional to dramaturgy.\n",
    "* divised into more or less short fragments.\n",
    "* open form $\\rightarrow$ fragments can be triggered in non-linear sequences (slightly different than what was said about random generation).\n",
    "* differenze musicali rispetto ai brani acusmatici.\n",
    "\n",
    "Anonymous - Dance! (2010).\n",
    "* stereo digital tape.\n",
    "* music functional to movement or dramatic timing.\n",
    "* second sensitive plane.\n",
    "* passive listening.\n",
    "* perceived as a complement to visual information.\n",
    "\n",
    "<audio controls src='suoni/dance.mp3'></audio>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2ca0f7",
   "metadata": {},
   "source": [
    "### Sound design for gaming <a id='game'></a>\n",
    "\n",
    "<!---<center><img src=\"img/game.png\" width=\"12%\"></center>--->\n",
    "\n",
    "<div style=\"width:12%; margin-left: auto; margin-right: auto\">\n",
    "\n",
    "![](/Users/andreavigani/Desktop/GHub/EMC/2_instruments/img/game.png)\n",
    "   \n",
    "</div>\n",
    "\n",
    "In this case too the sound must be functional to the game\n",
    "\n",
    "It should:\n",
    "\n",
    "* suggest a mood, evoke a feeling\n",
    "* indicate a geographical locale\n",
    "* define a character\n",
    "* mirror or exaggerate how things sound in real life\n",
    "* clarify the narrative\n",
    "\n",
    "To simplify there are five sounds categories\n",
    "\n",
    "* DX = Dialogue $\\rightarrow$ any verbal speech in the game (player talk).\n",
    "* MX = Music $\\rightarrow$ any non-diegetic music (orchestral music).\n",
    "* SFX = Sound Effects (Hard Effects) $\\rightarrow$ any sound from an real-life object (sounds of rocks falling).\n",
    "* FOL = Foley $\\rightarrow$ any sound effect that the player makes (footsteps, etc.).\n",
    "* BG = Backgrounds (ambience) $\\rightarrow$ noise from the environment (wind, rain, etc.).\n",
    "\n",
    "Procedural audio: \n",
    "\n",
    "* sounfiles triggered by user actions\n",
    "* real time shynthesis by internal oscillators.\n",
    "\n",
    "Artistically we could merge game music, soundscape composition, acousmatic music and radiodrama.\n",
    "\n",
    "Various - 8 bit music (1990).\n",
    "\n",
    "* stereo digital samples or real time oscillators.\n",
    "* music functional to gamer emotions.\n",
    "* also called procedural music.\n",
    "* pattern iteration.\n",
    "* music environment.\n",
    "* time dilation (psychological time)). \n",
    "* non-immersive audio diffusion but immersive psychological function.\n",
    "* passive listening.\n",
    "\n",
    "<audio controls src='suoni/gaming.mp3'></audio>\n",
    "\n",
    "[More info](img/gaming.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e732af",
   "metadata": {},
   "source": [
    "# Interactive systems <a id='interact'></a>\n",
    "\n",
    "This category includes genres that:\n",
    "\n",
    "* involve human interaction during the performance as happens with acoustic instruments.\n",
    "* these interactions influence the musical or sound text at different levels.\n",
    "\n",
    "Compared to the previous typology the presence of one or more performers significantly influences:\n",
    "\n",
    "* the anthropological and contextualised perception of the musical action\n",
    "* its representative nature.\n",
    "\n",
    "This category can be divided into three further subcategories:\n",
    "\n",
    "* hyper-instruments.\n",
    "* live set.\n",
    "* live coding.\n",
    "\n",
    "The image shows the possibles audio chains in this category.\n",
    "\n",
    "* real-time processing of pre-recorded or pre-synthesized sounds (parameters can be controlled through various types of devices - MIDI, OSC, HID, sensors).\n",
    "* real-time processing of signals captured through microphones or sensors (parameters can be controlled (parameters can be controlled in the previous modes but also through control signals derived in some way from the incoming audio signal).\n",
    "\n",
    "<!---<center><img src=\"img/chain2.png\" width=\"90%\"></center>--->\n",
    "\n",
    "<div style=\"width:90%; margin-left: auto; margin-right: auto\">\n",
    "\n",
    "![](/Users/andreavigani/Desktop/GHub/EMC/2_instruments/img/chain2.png)\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c601144",
   "metadata": {},
   "source": [
    "## Hyper-instrument <a id='hyper'></a>\n",
    "\n",
    "<!---<center><img src=\"img/lel.png\" width=\"7\"></center>--->\n",
    "\n",
    "<div style=\"width:7%; margin-left: auto; margin-right: auto\">\n",
    "\n",
    "![](/Users/andreavigani/Desktop/GHub/EMC/2_instruments/img/lel.png)\n",
    "   \n",
    "</div>\n",
    "<center><video width=\"60%\" controls src=\"suoni/boulez1.mp4\"></video></center>\n",
    "\n",
    "Augmented musical instrument that uses technology to extend the capabilities of a traditional instrument and enhance the performer's expressiveness.\n",
    "\n",
    "Two types:\n",
    "\n",
    "* Real-time augmented instrumental techniques $\\rightarrow$ sound processes are controlled by:\n",
    "  - an electronic performer who interacts with different types of controllers.  \n",
    "  - by information extracted from the audio signal produced by the instrument itself (feature extraction as control signals).\n",
    "* Cyborg luthiery $\\rightarrow$ sound processing is controlled by movements and/or gestures of the performer captured by different types of sensors.\n",
    "\n",
    "Three main goals:\n",
    "\n",
    "* respond to the performer's input in a dynamic and expressive way allowing for a deeper connection between the musician and the music. \n",
    "* produce sounds and musical effects that are impossible to achieve with traditional instruments alone. \n",
    "* expanding musical forms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aef3b1a",
   "metadata": {},
   "source": [
    "## Live set  <a id='lset'></a>\n",
    "\n",
    "<!---<center><img src=\"img/lset.png\" width=\"10%\"></center>--->\n",
    "\n",
    "<div style=\"width:10%; margin-left: auto; margin-right: auto\">\n",
    "\n",
    "![](/Users/andreavigani/Desktop/GHub/EMC/2_instruments/img/lset.png)\n",
    "   \n",
    "</div>\n",
    "<center><video width=\"60%\" controls src=\"suoni/slork1.mp4\"></video></center>\n",
    "\n",
    "Live performance where songs are performed using analog or digital electronic musical instruments.\n",
    "\n",
    "Often musicians improvise in a live set, making each performance unique.\n",
    "\n",
    "Can be a solo performance or a laptop ensemble. \n",
    "\n",
    "One of the most important aspects of this genre is to think about and assemble original performance environments which leads to the search for new human/machine interfaces dedicated to the generation of new sounds in the musical field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744b6882",
   "metadata": {},
   "source": [
    "## Live coding  <a id='lcoding'></a>\n",
    "\n",
    "<!---<center><img src=\"img/lcod.png\" width=\"19%\"></center>--->\n",
    "\n",
    "<div style=\"width:10%; margin-left: auto; margin-right: auto\">\n",
    "\n",
    "![](/Users/andreavigani/Desktop/GHub/EMC/2_instruments/img/lcod.png)\n",
    "   \n",
    "</div>\n",
    "<center><video width=\"60%\" controls src=\"suoni/livec1.mp4\"></video></center>\n",
    "\n",
    "Also called:\n",
    "\n",
    "* on-the-fly programming. \n",
    "* just in time programming\n",
    "* conversational programming\n",
    "\n",
    "Search for new musical forms, new places for performance and new audiences.\n",
    "\n",
    "It is part of a larger artistic movement called creative coding.\n",
    "\n",
    "If channeled into the Western musical tradition, we can think of it as a new version of organ improvisations in the Baroque era.\n",
    "\n",
    "From a semiographic point of view the most interesting aspect is that the score is the code and is created and manipulated in real time.\n",
    "\n",
    "The compositional process which has always belonged to the author's private sphere becomes public and an integral part of the performance itself (do you remember the first paragraphs of the previous chapter?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bdced7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# The virtual instrument paradigm <a id='virtual'></a> \n",
    "\n",
    "In this section we define a virtual instrument model that will be useful in next chapters.\n",
    "\n",
    "If anyone is unfamiliar with SuperCollider, they can download an introductory text at [this link](https://ccrma.stanford.edu/~ruviaro/texts/A_Gentle_Introduction_To_SuperCollider.pdf).\n",
    "\n",
    "In the chapter on live electronics we'll make some small modifications to allow signal input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6deddd49",
   "metadata": {},
   "source": [
    "## Software installation <a id='installazione'></a> \n",
    "\n",
    "Download and install:\n",
    "\n",
    "* [sc_kernel](https://github.com/capital-G/sc_kernel) (if you want to execute supercollider code in this notebook)\n",
    "* [SuperCollider](https://supercollider.github.io/) \n",
    "\n",
    "If you want to play SuperCollider from this notebook you have to select the 'sc_kernel'.\n",
    "\n",
    "Instead, I recommend copying and pasting the code into the SuperCollider IDE Interpreter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43310e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Instrumental model <a id='strum'></a> \n",
    "\n",
    "Let's create a computer model that contains functions common to most virtual instruments (sound generators).\n",
    "\n",
    "* an oscillator.\n",
    "* an amplitude envelope (with or without sustain).\n",
    "* a stereo panner.\n",
    "* a signal output.\n",
    "\n",
    "We also establish the basic parameters we want to control dynamically (they will increase depending on the synthesis algorithm chosen).\n",
    "\n",
    "* frequency (Hz).\n",
    "* general amplitude (0.0 to 1.0).\n",
    "* duration (seconds).\n",
    "* pan position (-1.0 to +1-0).\n",
    "* pan interpolation time (movements)\n",
    "* output bus.\n",
    "* trigger.\n",
    "* kind of voice allocation (doneAction:0 or 2).\n",
    "\n",
    "<!---<center><img src=\"img/modello.png\" width=\"30%\"></center>--->\n",
    "\n",
    "<div style=\"width:35%; margin-left: auto; margin-right: auto\">\n",
    "\n",
    "![](/Users/andreavigani/Desktop/GHub/EMC/2_instruments/img/modello.png)\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da90900",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> a Function"
     ]
    }
   ],
   "source": [
    "s.boot;\n",
    "{s.meter;\n",
    " s.scope;\n",
    " s.plotTree}.defer(2);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e8ef43",
   "metadata": {},
   "source": [
    "In SuperCollider we can define it with the 'SynthDef' (synthesis definition) class.\n",
    "\n",
    "This class contains the instructions (algorithms) needed to build multiple copies (instances) derived from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ce37e1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> a SynthDef"
     ]
    }
   ],
   "source": [
    "SynthDef.new(\\ciccio,  {arg freq=500, amp=0, dur=1, pan=0, pantime(0.0), \n",
    "                            out=0, t_gate=0, done=2;\n",
    "                        var sig, env;\n",
    "                            sig = SinOsc.ar(freq);\n",
    "                            env = Env.new([0.0,1.0,0.5,0.7,0],                \n",
    "                                          [0.1,0.1,0.3,1.0].normalizeSum * dur, \n",
    "                                          -4    \n",
    "                                          );\n",
    "\n",
    "                            env = EnvGen.kr(env, t_gate, doneAction:done);\n",
    "                            sig = sig * env * amp;\n",
    "                            sig = Pan2.ar(sig, pan.varlag(pantime));\n",
    "                            Out.ar(out, sig)\n",
    "                        }).add;         // Send the model to the Server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbb65be",
   "metadata": {},
   "source": [
    "Now we can generate as many instances as we want.\n",
    "\n",
    "We have two ways to control the instance parameters. \n",
    "\n",
    "They vary depending on the voice allocation we want.\n",
    "\n",
    "#### Dynamic voice allocation (poliphonic) \n",
    "\n",
    "Instance self-destructs when the amplitude envelope expires (doneAction:2).\n",
    "    \n",
    "In this case we pass parameters at instantiation time (array of \\\\key, value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61e37dc9",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Synth('ciccio' : 1003)"
     ]
    }
   ],
   "source": [
    "Synth.new(\\ciccio, [\\freq, rrand(890,1234), \n",
    "                    \\amp, 0.5, \n",
    "                    \\dur, rrand(0.1,2), \n",
    "                    \\pan, rand2(1.0), \n",
    "                    \\pantime, 0.1, \n",
    "                    \\done, 2, \n",
    "                    \\t_gate,1 ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c642d56f",
   "metadata": {},
   "source": [
    "\n",
    "#### Static voice allocation (monophnic) \n",
    "\n",
    "two passages:\n",
    "\n",
    "* create instance and assign it to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940a35ac",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "a = Synth.new(\\ciccio, [\\done, 0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24a63f0",
   "metadata": {},
   "source": [
    "* send one or multiple parameter changes by '.set' method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5c34bb",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "a.set(\\freq, rrand(890,1234), \n",
    "      \\amp, 0.5, \n",
    "      \\dur, rrand(0.1,2), \n",
    "      \\pan, rand2(1.0), \n",
    "      \\pantime, 0.1, \n",
    "      \\t_gate,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3986af97",
   "metadata": {},
   "source": [
    "Then you can destroy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105cc33",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "a.free;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7aea89",
   "metadata": {},
   "source": [
    "Or destroy all Synths on the Server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538fca0a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "s.freeAll;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf788d",
   "metadata": {},
   "source": [
    "The type of voice allocation is strictly linked to the type of amplitude envelope of the instrumental model.\n",
    "\n",
    "#### With sustain (as in keyboards) \n",
    "\n",
    "There isn't a total duration.\n",
    "\n",
    "In this case we must send a message of 'gate 1' (noteon) and then 'gate 0' (noteoff)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa45964",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "SynthDef.new(\\ciccio,  {arg freq=500, amp=0, gate=0, done=2;   // Siplified \n",
    "                        var sig, env;\n",
    "                            sig = SinOsc.ar(freq);\n",
    "                            env = Env.adsr;         // A sustained envelope\n",
    "                            env = EnvGen.kr(env, gate, doneAction:done);\n",
    "                            sig = sig * env * amp;\n",
    "                            sig = Pan2.ar(sig, 0);\n",
    "                        Out.ar(0, sig)\n",
    "                        }).add;                               \n",
    "\n",
    "a = Synth.new(\\ciccio, [\\done, 0]); // Create synth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa021079",
   "metadata": {},
   "source": [
    "gate 1 (noteon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9684a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "a.set(\\amp, 0.5, \\gate, 1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0e565f",
   "metadata": {},
   "source": [
    "gate 0 (noteoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202c719",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "a.set(\\gate, 0);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf382d44",
   "metadata": {},
   "source": [
    "destroy the Synth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971945f3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "a.free;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c1acc",
   "metadata": {},
   "source": [
    "#### Without sustain (as in percussion or plucked intruments) \n",
    "\n",
    "There is a total duration.\n",
    "\n",
    "In this case we must send a gate message preceded by 't_' - \n",
    "\n",
    "This produce an automatic 'gate 0' message after a time calculated on the specified duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a12376a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "SynthDef.new(\\ciccio,  {arg freq=500, amp=0, dur=1, t_gate=0, done=2; \n",
    "                        var sig, env;\n",
    "                            sig = SinOsc.ar(freq);\n",
    "                            env = Env.perc(0.1*dur,0.9*dur);                     \n",
    "                            env = EnvGen.kr(env, t_gate, doneAction:done);\n",
    "                            sig = sig * env * amp;\n",
    "                            sig = Pan2.ar(sig, 0);\n",
    "                        Out.ar(0, sig)\n",
    "                        }).add;                                         \n",
    "\n",
    "a = Synth.new(\\ciccio, [\\done, 0]);    // Create the Synth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ffa077",
   "metadata": {},
   "source": [
    "t_gate 1 (noteon + noteoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab58772e",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "a.set(\\amp, 0.5, \\t_gate,1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e77003b",
   "metadata": {},
   "source": [
    "You can also trigger and destroy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e5333b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "a.set(\\amp, 0.5, \\done, 2, \\t_gate,1); // Destroy it after the amp envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934602bd",
   "metadata": {},
   "source": [
    "Modifications to this model will be addressed in specific chapters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SuperCollider",
   "language": "",
   "name": "sc_kernel"
  },
  "language_info": {
   "file_extension": ".scd",
   "mimetype": "text/x-sclang",
   "name": "smalltalk",
   "pygments_lexer": "supercollider"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
